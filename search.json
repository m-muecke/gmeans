[{"path":"https://m-muecke.github.io/gmeans/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 gmeans authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://m-muecke.github.io/gmeans/articles/introduction.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Why G-means?","text":"illustrate purpose G-means algorithm, let’s start adapted k-means clustering example tidymodels website. example shows challenge determining number clusters (k) clustering analysis. Throughout vignette, use data.table data manipulation, custom tidy, augment, glance functions handling model output, inspired broom package functionality. begin generating random two-dimensional data naturally forms three clusters. cluster’s data comes different multivariate Gaussian distribution unique means:  simple example, know three clusters. However, real-world scenarios, number clusters often unknown must determined part analysis.","code":"set.seed(27)  centers <- data.table(   cluster = factor(1:3),   num_points = c(100, 150, 50),   x1 = c(5, 0, -3),   x2 = c(-1, 1, -2) ) points <- centers[, .(   x1 = rnorm(num_points, mean = x1),   x2 = rnorm(num_points, mean = x2) ), by = cluster]  ggplot(points, aes(x1, x2, color = cluster)) +   geom_point(alpha = 0.3)"},{"path":"https://m-muecke.github.io/gmeans/articles/introduction.html","id":"challenges-with-k-means","dir":"Articles","previous_headings":"","what":"Challenges with k-means","title":"Why G-means?","text":"k-means clustering requires specifying number clusters, k, beforehand. illustrate , let’s fit k-means model k = 3: , fit k-means model correct number clusters know true structure data. However, knowledge often available practice. explore effect different k, can fit k-means models varying numbers clusters visualize results. make handling k-means output easier, define tidy, augment, glance functions mimic functionality broom package: augment() function adds cluster assignments original dataset, allowing us see data point classified: tidy() function provides per-cluster summary, displaying cluster centers, sizes, within-cluster sum squares: obtain single-row summary overall metrics total sum squares number iterations, use glance() function: Using helper functions, can easily extract manipulate results k-means clustering different values k:","code":"points <- points[, cluster := NULL] kclust <- kmeans(points, centers = 3) kclust #> K-means clustering with 3 clusters of sizes 146, 53, 101 #>  #> Cluster means: #>           x1         x2 #> 1 -0.1277535  1.1366932 #> 2 -2.9430301 -1.9877357 #> 3  5.0156304 -0.8637111 #>  #> Clustering vector: #>   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 #>  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 #>  [75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 #> [112] 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 #> [149] 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #> [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #> [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 #> [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 #> [297] 2 2 2 2 #>  #> Within cluster sum of squares by cluster: #> [1] 307.3195 119.2575 213.5906 #>  (between_SS / total_SS =  82.9 %) #>  #> Available components: #>  #> [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\" #> [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\" `%||%` <- function(x, y) if (!is.null(x)) x else y  tidy <- function(x, col.names = colnames(x$centers)) {   col.names <- col.names %||% paste0(\"x\", seq_len(ncol(x$centers)))   dt <- as.data.table(x$centers)   setnames(dt, col.names)   dt[, let(     size = x$size,     withinss = x$withinss,     cluster = factor(seq_len(.N))   )][] }  augment <- function(x, data) {   if (inherits(data, \"matrix\") && is.null(colnames(data))) {     colnames(data) <- paste0(\"X\", seq_len(ncol(data)))   }   dt <- as.data.table(data)   dt[, .cluster := as.factor(x$cluster)][] }  glance <- function(x) {   as.data.table(x[c(\"totss\", \"tot.withinss\", \"betweenss\", \"iter\")]) } augment(kclust, points) #>             x1         x2 .cluster #>          <num>      <num>   <fctr> #>   1:  6.907163 -0.5580268        3 #>   2:  6.144877  0.6338766        3 #>   3:  4.235469 -1.1313761        3 #>  ---                               #> 298: -3.486164 -1.2307705        2 #> 299: -1.439651 -3.7284605        2 #> 300: -3.531135 -0.7172739        2 tidy(kclust) #>            x1         x2  size withinss cluster #>         <num>      <num> <int>    <num>  <fctr> #> 1: -0.1277535  1.1366932   146 307.3195       1 #> 2: -2.9430301 -1.9877357    53 119.2575       2 #> 3:  5.0156304 -0.8637111   101 213.5906       3 glance(kclust) #>       totss tot.withinss betweenss  iter #>       <num>        <num>     <num> <int> #> 1: 3746.156     640.1676  3105.989     2 kclusts <- data.table(k = 1:9) kclusts[, kclust := lapply(k, \\(x) kmeans(points, x))] kclusts[, let(   tidied = lapply(kclust, tidy),   glanced = lapply(kclust, glance),   augmented = lapply(kclust, augment, points) )]  clusters <- kclusts[, .(k, rbindlist(tidied))] assignments <- kclusts[, .(k, rbindlist(augmented))] clusterings <- kclusts[, .(k, rbindlist(glanced))]  p1 <- ggplot(assignments, aes(x = x1, y = x2)) +   geom_point(aes(color = .cluster), alpha = 0.8) +   facet_wrap(~k) +   labs(title = \"k-means Clustering Results with Different Values of k\") p1"},{"path":"https://m-muecke.github.io/gmeans/articles/introduction.html","id":"visualizing-cluster-centers","dir":"Articles","previous_headings":"","what":"Visualizing Cluster Centers","title":"Why G-means?","text":"enhance visualization, let’s add cluster centers:","code":"p2 <- p1 + geom_point(data = clusters, size = 10, shape = \"x\") +   labs(title = \"k-means Clustering with Centers\") p2"},{"path":"https://m-muecke.github.io/gmeans/articles/introduction.html","id":"evaluating-clustering-performance","dir":"Articles","previous_headings":"","what":"Evaluating Clustering Performance","title":"Why G-means?","text":"Finally, can look total within-cluster sum squares (WSS) changes different values k. helps us see well data clustered k increases:  general, WSS decreases number clusters k increases, expected since clusters usually results better fit. However, often look point plot decrease WSS starts slow , creating noticeable “elbow”. elbow suggests adding clusters beyond point offers little improvement, indicating good number clusters. example, bend around k = 3, suggesting three clusters capture main structure data effectively.","code":"ggplot(clusterings, aes(k, tot.withinss)) +   geom_line() +   geom_point() +   labs(     title = \"Total Within-Cluster Sum of Squares vs. Number of Clusters (k)\",     x = \"Number of Clusters (k)\",     y = \"Total Within-Cluster Sum of Squares\"   )"},{"path":"https://m-muecke.github.io/gmeans/articles/introduction.html","id":"motivation-for-g-means","dir":"Articles","previous_headings":"","what":"Motivation for G-means","title":"Why G-means?","text":"seen plots, choosing right number clusters straightforward. use metrics like WSS help decide, methods can subjective prone error. G-means algorithm comes , automatically determines number clusters assessing data distribution within cluster. using statistical hypothesis testing (Anderson-Darling test implementation), G-means provides robust automated way find “correct” number clusters. next section, ’ll see use G-means explore benefits traditional k-means clustering.","code":""},{"path":"https://m-muecke.github.io/gmeans/articles/introduction.html","id":"g-means","dir":"Articles","previous_headings":"","what":"G-means","title":"Why G-means?","text":"Let’s now apply G-means algorithm data: expected previous analysis, G-means identifies 3 clusters, aligning elbow point observed WSS plot. Next, let’s explore G-means performs different dataset: Since gmeans() uses stats::kmeans() hood, can use previously defined helper functions analyze clustering results. augment() function adds cluster assignments original dataset easy plotting:  tidy() function provides summary cluster: glance() function gives overall summary model:","code":"set.seed(123)  gmeans(points) #> K-means clustering with 3 clusters of sizes 53, 146, 101 #>  #> Cluster means: #>           x1         x2 #> 1 -2.9430301 -1.9877357 #> 2 -0.1277535  1.1366932 #> 3  5.0156304 -0.8637111 #>  #> Clustering vector: #>   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 #>  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 #>  [75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 #> [112] 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 #> [149] 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 #> [186] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 #> [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 1 1 1 1 1 1 1 1 1 #> [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #> [297] 1 1 1 1 #>  #> Within cluster sum of squares by cluster: #> [1] 119.2575 307.3195 213.5906 #>  (between_SS / total_SS =  82.9 %) #>  #> Available components: #>  #> [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\" #> [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\" set.seed(1234)  x <- as.matrix(iris[, -5]) gclust <- gmeans(x) augment(gclust, x) |>   ggplot(aes(x = Petal.Length, y = Petal.Width)) +   geom_point(aes(color = .cluster)) tidy(gclust) #>    Sepal.Length Sepal.Width Petal.Length Petal.Width  size  withinss cluster #>           <num>       <num>        <num>       <num> <int>     <num>  <fctr> #> 1:     5.005660    3.369811     1.560377    0.290566    53  28.55208       1 #> 2:     6.301031    2.886598     4.958763    1.695876    97 123.79588       2 glance(gclust) #>       totss tot.withinss betweenss  iter #>       <num>        <num>     <num> <int> #> 1: 681.3706      152.348  529.0226     1"},{"path":"https://m-muecke.github.io/gmeans/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Maximilian Mücke. Author, maintainer.","code":""},{"path":"https://m-muecke.github.io/gmeans/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mücke M (2024). gmeans: G-means Clustering. R package version 0.0.1, https://github.com/m-muecke/gmeans, https://m-muecke.github.io/gmeans/.","code":"@Manual{,   title = {gmeans: G-means Clustering},   author = {Maximilian Mücke},   year = {2024},   note = {R package version 0.0.1,     https://github.com/m-muecke/gmeans},   url = {https://m-muecke.github.io/gmeans/}, }"},{"path":"https://m-muecke.github.io/gmeans/index.html","id":"gmeans","dir":"","previous_headings":"","what":"G-means Clustering","title":"G-means Clustering","text":"aim package provide implementation G-means algorithm R. G-means algorithm clustering algorithm extends k-means algorithm automatically determining number clusters. algorithm introduced Hamerly Elkan (2003).","code":""},{"path":"https://m-muecke.github.io/gmeans/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"G-means Clustering","text":"can install development version gmeans GitHub :","code":"# install.packages(\"pak\") pak::pak(\"m-muecke/gmeans\")"},{"path":"https://m-muecke.github.io/gmeans/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"G-means Clustering","text":"","code":"library(gmeans)  km <- gmeans(mtcars) km #> K-means clustering with 2 clusters of sizes 18, 14 #>  #> Cluster means: #>        mpg      cyl     disp        hp     drat       wt     qsec        vs #> 1 23.97222 4.777778 135.5389  98.05556 3.882222 2.609056 18.68611 0.7777778 #> 2 15.10000 8.000000 353.1000 209.21429 3.229286 3.999214 16.77214 0.0000000 #>          am     gear     carb #> 1 0.6111111 4.000000 2.277778 #> 2 0.1428571 3.285714 3.500000 #>  #> Clustering vector: #>           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive  #>                   1                   1                   1                   1  #>   Hornet Sportabout             Valiant          Duster 360           Merc 240D  #>                   2                   1                   2                   1  #>            Merc 230            Merc 280           Merc 280C          Merc 450SE  #>                   1                   1                   1                   2  #>          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental  #>                   2                   2                   2                   2  #>   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #>                   2                   1                   1                   1  #>       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28  #>                   1                   2                   2                   2  #>    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa  #>                   2                   1                   1                   1  #>      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E  #>                   2                   1                   2                   1  #>  #> Within cluster sum of squares by cluster: #> [1] 58920.54 93643.90 #>  (between_SS / total_SS =  75.5 %) #>  #> Available components: #>  #> [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\" #> [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""},{"path":"https://m-muecke.github.io/gmeans/index.html","id":"related-work","dir":"","previous_headings":"","what":"Related work","title":"G-means Clustering","text":"nortest: R package testing composite hypothesis normality.","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/ad.test.html","id":null,"dir":"Reference","previous_headings":"","what":"Anderson-Darling Normality Test — ad.test","title":"Anderson-Darling Normality Test — ad.test","text":"Perform Anderson-Darling normality test.","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/ad.test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Anderson-Darling Normality Test — ad.test","text":"","code":"ad.test(x)"},{"path":"https://m-muecke.github.io/gmeans/reference/ad.test.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Anderson-Darling Normality Test — ad.test","text":"Adapted nortest::ad.test()","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/ad.test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Anderson-Darling Normality Test — ad.test","text":"x numeric() vector data values. Missing values allowed, number non-missing values must greater 7.","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/ad.test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Anderson-Darling Normality Test — ad.test","text":"list inheriting classes \"htest\" containing following components: statistic: value statistic. p.value: p-value test. method: character string \"Anderson-Darling normality test\". data.name: character string giving name(s) data.","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/ad.test.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Anderson-Darling Normality Test — ad.test","text":"Anderson-Darling test EDF omnibus test composite hypothesis normality. test statistic $$   ^2 = -n -\\frac{1}{n} \\sum_{=1}^{n} (2i - 1) [\\ln(z_{}) + \\ln(1 - z_{n + 1 - })] $$ \\(z_{} = \\Phi(\\frac{x_{} - \\bar{x}}{s})\\). , \\(\\Phi\\) cumulative distribution function standard normal distribution, \\(\\bar{x}\\) \\(s\\) mean standard deviation data values. p-value computed modified statistic \\(^2_*=^2 (1.0 + 0.75/n + 2.25/n^{2})\\) according Table 4.9 Stephens (1986).","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/ad.test.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Anderson-Darling Normality Test — ad.test","text":"Stephens, . M (1986). “Goodness--Fit-Techniques.” D'Agostino, B. R (eds.), chapter Tests based EDF statistics. CRC Press. Thode, C. H (2002). Testing normality, 1 edition. CRC Press. doi:10.1201/9780203910894 .","code":""},{"path":[]},{"path":"https://m-muecke.github.io/gmeans/reference/ad.test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Anderson-Darling Normality Test — ad.test","text":"","code":"set.seed(123) ad.test(rnorm(100, mean = 5, sd = 3)) #>  #> \tAnderson-Darling normality test #>  #> data:  rnorm(100, mean = 5, sd = 3) #> A = 0.182, p-value = 0.9104 #>  ad.test(runif(100, min = 2, max = 4)) #>  #> \tAnderson-Darling normality test #>  #> data:  runif(100, min = 2, max = 4) #> A = 1.3941, p-value = 0.001244 #>"},{"path":"https://m-muecke.github.io/gmeans/reference/compute_wss.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Within-Cluster Sum of Squares — compute_wss","title":"Compute Within-Cluster Sum of Squares — compute_wss","text":"Compute Within-Cluster Sum Squares","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/compute_wss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Within-Cluster Sum of Squares — compute_wss","text":"","code":"compute_wss(object, newdata = NULL)"},{"path":"https://m-muecke.github.io/gmeans/reference/compute_wss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Within-Cluster Sum of Squares — compute_wss","text":"object class inheriting \"kmeans\". newdata matrix() new data predict .","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/compute_wss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Within-Cluster Sum of Squares — compute_wss","text":"WSS defined $$   \\sum_{=1}^{n} \\left\\|x_{} - \\mu_{j()}\\right\\|^2 $$, \\(x_{}\\) data point \\(\\mu_{j()}\\) centroid cluster \\(x_{}\\) assigned. new data provided, function predicts nearest cluster new observation computes WSS points based predicted clusters.","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/compute_wss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Within-Cluster Sum of Squares — compute_wss","text":"","code":"km <- kmeans(mtcars, 5) compute_wss(km) #> [1]   208.0365  6355.5813 46659.3172  3616.8297  7256.4492 # or with new data compute_wss(km, mtcars) #> [1]   208.0365  6355.5813 46659.3172  3616.8297  7256.4492"},{"path":"https://m-muecke.github.io/gmeans/reference/gmeans.html","id":null,"dir":"Reference","previous_headings":"","what":"G-means Clustering — gmeans","title":"G-means Clustering — gmeans","text":"Perform G-means clustering data matrix.","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/gmeans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"G-means Clustering — gmeans","text":"","code":"gmeans(x, k_init = 2L, k_max = 10L, level = 0.05, ...)"},{"path":"https://m-muecke.github.io/gmeans/reference/gmeans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"G-means Clustering — gmeans","text":"x numeric matrix data, object can coerced matrix (numeric vector data frame numeric columns). k_init integer(1) initial amount centers. Default 2L. k_max integer(1) maximum amount centers. Default 10L. level numeric(1) significance level Anderson-Darling test. Default 0.05. See ad.test() information. ... additional arguments passed stats::kmeans().","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/gmeans.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"G-means Clustering — gmeans","text":"G-means clustering algorithm extension traditional k-means algorithm automatically determines number clusters iteratively testing Gaussianity data within clusters. process begins specified initial number clusters (k_init) iteratively increases number clusters reaches specified maximum (k_max) data within clusters determined Gaussian specified significance level (level). algorithm outlined follows: Let \\(C\\) initial set centers (usually \\(C \\leftarrow \\{\\bar{x}\\}\\)). Perform k-means clustering dataset \\(X\\) using current set centers \\(C\\), .e., \\(C \\leftarrow \\text{kmeans}(C, X)\\). center \\(c_j\\), identify set data points \\(\\{x_i \\mid \\text{class}(x_i) = j\\}\\) assigned \\(c_j\\). Use Anderson-Darling test check set data points \\(\\{x_i \\mid \\text{class}(x_i) = j\\}\\) follows Gaussian distribution data points appear Gaussian, keep \\(c_j\\). confidence level \\(\\alpha\\). Otherwise, replace \\(c_j\\) two new centers. Repeat step 2 centers added.","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/gmeans.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"G-means Clustering — gmeans","text":"Hamerly, Greg, Elkan, Charles (2003). “Learning k k-means.” Thrun S, Saul L, Schölkopf B (eds.), Advances Neural Information Processing Systems, volume 16. https://proceedings.neurips.cc/paper_files/paper/2003/file/234833147b97bb6aed53a8f4f1c7a7d8-Paper.pdf.","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/gmeans.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"G-means Clustering — gmeans","text":"","code":"set.seed(123) x <- rbind(   matrix(rnorm(100, sd = 0.3), ncol = 2),   matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2) ) colnames(x) <- c(\"x\", \"y\") cl <- gmeans(x)"},{"path":"https://m-muecke.github.io/gmeans/reference/predict.kmeans.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for G-means Clustering — predict.kmeans","title":"Predict Method for G-means Clustering — predict.kmeans","text":"Predicted values based G-means clustering model.","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/predict.kmeans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for G-means Clustering — predict.kmeans","text":"","code":"# S3 method for class 'kmeans' predict(   object,   newdata,   method = c(\"euclidean\", \"manhatten\", \"minkowski\"),   p = 2,   ... )"},{"path":"https://m-muecke.github.io/gmeans/reference/predict.kmeans.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Predict Method for G-means Clustering — predict.kmeans","text":"Adapted clue","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/predict.kmeans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for G-means Clustering — predict.kmeans","text":"object class inheriting \"kmeans\". newdata matrix() new data predict . method character(1) distance metric use. Either \"euclidean\", \"manhattan\", \"minkowski\". Default \"euclidean\". p numeric(1) power Minkowski distance. Default 2. ... additional arguments.","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/predict.kmeans.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for G-means Clustering — predict.kmeans","text":"predict method G-means clustering assigns new data points nearest cluster center identified G-means algorithm. method uses specified distance metric calculate distance new data point cluster centers, assigns point cluster closest center. method argument specifies distance metric use. following options: \"euclidean\": euclidean distance default metric used k-means defined $$     d(x, y) = \\sqrt{\\sum_{=1}^{n} (x_i - y_i)^2}   $$ \"manhatten\": Manhatten distance defined $$     d(x, y) = \\sum_{=1}^{n} |x_i - y_i|   $$ \"minkowski\": Minkowski distance defined $$     d(x, y) = \\left( \\sum_{=1}^{n} |x_i - y_i|^p \\right)^{1/p},   $$ \\(p\\) parameter defines distance type (e.g., \\(p=2\\) Euclidean, \\(p=1\\) Manhattan).","code":""},{"path":"https://m-muecke.github.io/gmeans/reference/predict.kmeans.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for G-means Clustering — predict.kmeans","text":"","code":"set.seed(123) x <- as.matrix(iris[, -5]) cl <- gmeans(x)  newdata <- x[1:10, ] predict(cl, newdata) #>  [1] 6 9 9 9 6 1 9 6 4 9"}]
